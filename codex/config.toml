model = "gpt-oss-120b"
model_reasoning_effort = "high"
hide_agent_reasoning = false
network_access = true

[tools]
web_search = true

[model_providers.ollama]
name = "Ollama"
base_url = "http://127.0.0.1:11434/v1" # or set CODEX_OSS_BASE_URL

[profiles.gpt-oss-120b]
model = "gpt-oss-120b"
model_provider = "ollama"

[mcp_servers.serena]
command = "uvx"
args = ["--from", "git+https://github.com/oraios/serena", "serena", "start-mcp-server", "--context", "codex"]

[mcp_servers.fetch]
command = "uvx"
args = ["mcp-server-fetch"]

# [mcp_servers.github]
# url = "https://api.githubcopilot.com/mcp/"
# bearer_token_env_var = "GITHUB_PAT"

[mcp_servers.sequential-thinking]
command = "npx"
args = ["-y", "@modelcontextprotocol/server-sequential-thinking"]

[mcp_servers.filesystem]
command = "npx"
args = ["-y", "@modelcontextprotocol/server-filesystem"]

[mcp_servers.time]
command = "uvx"
args = ["mcp-server-time"]

[projects."/home/hori/works/work1/la-bench2025/tmp_codex"]
trust_level = "trusted"

[projects."/home/hori/works/work1/la-bench2025/la-bench2025_speckit"]
trust_level = "trusted"
